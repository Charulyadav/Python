{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8031dfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\charul\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\charul\\anaconda3\\lib\\site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\charul\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f025d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\charul\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\charul\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\charul\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\charul\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\charul\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81475063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\charul\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\charul\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\charul\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\charul\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\charul\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3a95f77",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 70\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m URL \u001b[38;5;241m=\u001b[39m get_next_page(soup)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     soup \u001b[38;5;241m=\u001b[39m getdata(URL)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'soup' is not defined"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def getdata(URL):\n",
    "    HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.4; x64; en-US) AppleWebKit/601.48 (KHTML, like Gecko) Chrome/49.0.2767.228 Safari/602.0 Edge/13.57484', 'Accept-Language': 'en-Us, en;q=0.5'}\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "    return soup\n",
    "\n",
    "def get_title(soup):\n",
    "    try:\n",
    "        title = soup.find(\"span\", attrs={\"id\":'productTitle'})\n",
    "        \n",
    "        title_value = title.text\n",
    "\n",
    "        title_string = title_value.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        title_string = \"\"\n",
    "\n",
    "    return title_string\n",
    "\n",
    "def get_price(soup):\n",
    "    try:\n",
    "        price = soup.find(\"span\", attrs={'class':'a-offscreen'}).string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "\n",
    "        try:\n",
    "            price = soup.find(\"span\", attrs={'id':'priceblock_dealprice'}).string.strip()\n",
    "\n",
    "        except:\n",
    "            price = \"\"\n",
    "\n",
    "    return price\n",
    "\n",
    "def get_rating(soup):\n",
    "    try:\n",
    "        rating = soup.find(\"i\", attrs={'class':'a-icon a-icon-star a-star-4-5'}).string.strip()\n",
    "    \n",
    "    except AttributeError:\n",
    "        try:\n",
    "            rating = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.strip()\n",
    "        except:\n",
    "            rating = \"\"\n",
    "\n",
    "    return rating\n",
    "\n",
    "def get_review_count(soup):\n",
    "    try:\n",
    "        review_count = soup.find(\"span\", attrs={'id':'acrCustomerReviewText'}).string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        review_count = \"\"\n",
    "\n",
    "    return review_count\n",
    "\n",
    "def get_next_page(soup):\n",
    "    page = soup.find('span', attrs={'class':'s-pagination-strip'})\n",
    "    \n",
    "    if not page.find('span', attrs={'class':'s-pagination-item s-pagination-next s-pagination-disabled '}):\n",
    "        URL = 'https://www.amazon.in/'+ str(page.find('span', attrs={'class':'s-pagination-item s-pagination-next s-pagination-button s-pagination-separator'}).find('a')['href'])\n",
    "        return URL\n",
    "    \n",
    "    else:\n",
    "        return\n",
    "    \n",
    "URL = get_next_page(soup)\n",
    "    \n",
    "while True:\n",
    "    soup = getdata(URL)\n",
    "    URL1 = getnextpage(soup)\n",
    "    if not URL1:\n",
    "        break\n",
    "    get_next_page(URL1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    starting_url = \"https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1\"\n",
    "    data = []\n",
    "\n",
    "    while starting_url:\n",
    "        soup = getdata(starting_url)\n",
    "        links = soup.find_all(\"a\", class_='a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal')\n",
    "\n",
    "        for link in links:\n",
    "            new_url = \"https://www.amazon.in\" + link.get('href')\n",
    "            new_soup = getdata(new_url)\n",
    "            title = get_title(new_soup)\n",
    "            price = get_price(new_soup)\n",
    "            rating = get_rating(new_soup)\n",
    "            review_count = get_review_count(new_soup)\n",
    "            data.append({'title': title, 'price': price, 'rating': rating, 'reviews': review_count})\n",
    "\n",
    "        starting_url = get_next_page(soup)\n",
    "\n",
    "    amazon_df = pd.DataFrame(data)\n",
    "    amazon_df.dropna(subset=['title'], inplace=True)\n",
    "    amazon_df.to_csv(\"amazon_bags_data.csv\", header=True, index=False)\n",
    "amazon_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f90f17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
